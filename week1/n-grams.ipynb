{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP5qIFD4ga80pMkKP8urebh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# What are N-Grams?\n","\n","**N-grams** language model is a probabilistic model that predicts the next word in a sequence based on the previous N - 1 words.\n"," <br>\n","\n","**Use cases**\n","\n","- Text prediction  \n","- Spelling correction  \n","\n","---\n","\n","## Understanding N-Grams\n","\n","The concept of an n-gram is straightforward: it is a sequence of 'n' consecutive items.\n","\n","- If **n = 1**, it's a **unigram**\n","- If **n = 2**, it's a **bigram**\n","- If **n = 3**, it's a **trigram**\n","- And so on...\n","\n","> The larger the value of 'n', the more context you capture, but this comes with diminishing returns due to computational cost and data sparsity.\n","\n","### Example\n","\n","Consider the sentence:\n","\n","> \"The quick brown fox jumps over the lazy dog.\"\n","\n","Here are some examples of n-grams:\n","\n","- **Unigrams**:  \n","  `The`, `quick`, `brown`, `fox`, `jumps`, `over`, `the`, `lazy`, `dog`\n","  \n","- **Bigrams**:  \n","  `The quick`, `quick brown`, `brown fox`, `fox jumps`, `jumps over`, `over the`, `the lazy`, `lazy dog`\n","  \n","- **Trigrams**:  \n","  `The quick brown`, `quick brown fox`, `brown fox jumps`, `fox jumps over`, `jumps over the`, `over the lazy`, `the lazy dog`\n","\n","> - **Unigrams** lack context  \n","> - **Bigrams** provide minimal context  \n","> - **Trigrams** start to form more meaningful phrases\n","\n","---\n","\n","\n","\n","$$\n","P(w_i | w_{i-1}) = \\frac{Count(w_{i-1})}{Count(w_{i-1}, w_i)}\n","$$\n","\n","This gives the most probable value of $P(w_i | w_{i-1})$ based on your training data.\n","\n","---\n","\n","\n","## Challenges with N-Grams\n","\n","Despite their usefulness, n-grams face several challenges:\n","\n","- **Data Sparsity**:  \n","  As 'n' increases, many n-grams may appear rarely or not at all, reducing the model's ability to generalize.\n","\n","- **Computational Complexity**:  \n","  Higher values of 'n' lead to an exponential increase in the number of possible combinations.\n","\n","- **Context Limitation**:  \n","  N-grams use a fixed-size context window and cannot capture long-range dependencies in language.\n","\n","---"],"metadata":{"id":"HLv4np8UytXP"}},{"cell_type":"markdown","source":["### Text classification\n","\n","https://www.kaggle.com/code/leekahwin/text-classification-using-n-gram-0-8-f1"],"metadata":{"id":"VMVBJ--E06OK"}},{"cell_type":"markdown","source":["## Text generation\n","\n","https://www.kaggle.com/code/dimitriirfan/text-generation-using-n-gram-model"],"metadata":{"id":"1sVeLzmE1JdF"}},{"cell_type":"markdown","source":["## Spell correction\n","\n","https://www.kaggle.com/code/dhruvdeshmukh/spelling-corrector-using-n-gram-language-model"],"metadata":{"id":"Yvx4pv7b1Zzx"}},{"cell_type":"code","source":["corpus = [\n","    \"i love deep learning\",\n","    \"i love machine learning\",\n","    \"i love coding\",\n","    \"deep learning is fun\",\n","    \"deep learning is amazing\",\n","    \"machine learning is powerful\",\n","    \"i love artificial intelligence\",\n","    \"i love deep neural networks\",\n","    \"i love programming\",\n","    \"coding is fun\",\n","    \"machine learning is interesting\",\n","    \"deep learning is the future\",\n","    \"machine learning and deep learning are related\",\n","    \"i enjoy deep learning\",\n","    \"i enjoy machine learning\",\n","    \"i enjoy coding\",\n","    \"deep learning has potential\",\n","    \"machine learning can solve problems\",\n","    \"coding makes me happy\",\n","    \"i am passionate about deep learning\",\n","    \"i am passionate about machine learning\",\n","    \"coding helps me think logically\",\n","    \"deep learning and machine learning are exciting\",\n","    \"artificial intelligence is evolving\",\n","    \"i love working with data\",\n","    \"deep learning powers modern applications\",\n","    \"machine learning can change the world\",\n","    \"i want to master deep learning\",\n","    \"i want to master machine learning\",\n","    \"coding is the future\",\n","    \"machine learning is changing industries\",\n","    \"deep learning has revolutionized AI\",\n","    \"deep learning models are complex\",\n","    \"i study machine learning\",\n","    \"i study deep learning\",\n","    \"deep learning requires a lot of data\",\n","    \"machine learning requires good data\",\n","    \"i love to learn deep learning\",\n","    \"i love to learn machine learning\",\n","    \"deep learning is used in many fields\",\n","    \"machine learning is widely applied\",\n","    \"coding challenges are fun\",\n","    \"i love building models with deep learning\",\n","    \"i love building models with machine learning\",\n","    \"i love creating algorithms\",\n","    \"deep learning is advancing rapidly\",\n","    \"machine learning is evolving quickly\",\n","    \"coding is a useful skill\",\n","    \"deep learning uses neural networks\",\n","    \"machine learning uses algorithms\",\n","    \"deep learning and coding go hand in hand\",\n","    \"i enjoy working on deep learning projects\",\n","    \"i enjoy working on machine learning projects\",\n","    \"deep learning helps with pattern recognition\",\n","    \"machine learning is about data analysis\",\n","    \"i am learning deep learning\",\n","    \"i am learning machine learning\",\n","    \"deep learning can predict outcomes\",\n","    \"machine learning can detect patterns\",\n","    \"coding helps to automate tasks\",\n","    \"deep learning is useful in many industries\",\n","    \"machine learning is becoming more popular\",\n","    \"deep learning needs a lot of computational power\",\n","    \"machine learning needs good algorithms\"\n","]"],"metadata":{"id":"MvC8f3PK08gD","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import List, Tuple\n","from collections import defaultdict\n","\n","def tokenize(corpus: List[str]) -> List[List[str]]:\n","    return [[\"<s>\"] + sentence.lower().split() + [\"</s>\"] for sentence in corpus]\n","\n","class NGramMLE:\n","    def __init__(self, corpus: List[str], n: int):\n","        self.n = n\n","        self.ngram_counts = defaultdict(int)\n","        self.context_counts = defaultdict(int)\n","        self._train(tokenize(corpus))\n","\n","    def _train(self, tokenized_corpus: List[List[str]]):\n","        for sentence in tokenized_corpus:\n","            sentence = [\"<s>\"] * (self.n - 1) + sentence  # Pad with start tokens\n","            for i in range(len(sentence) - self.n + 1):\n","                ngram = tuple(sentence[i:i + self.n])\n","                context = ngram[:-1]\n","                self.ngram_counts[ngram] += 1\n","                self.context_counts[context] += 1\n","\n","    def prob(self, context: Tuple[str, ...], word: str) -> float:\n","        if len(context) != self.n - 1:\n","            raise ValueError(f\"expected context of length {self.n - 1}\")\n","        ngram = context + (word,)\n","        context_count = self.context_counts[context]\n","        return self.ngram_counts[ngram] / context_count if context_count > 0 else 0.0\n","\n","    def generate_prob_table(self):\n","        for ngram, count in self.ngram_counts.items():\n","            context = ngram[:-1]\n","            word = ngram[-1]\n","            print(f\"P({word} | {context}) = {self.prob(context, word):.4f}\")"],"metadata":{"id":"cizZwjFxyf7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = NGramMLE(corpus, n=3)\n","\n","print(\"P(learning | love, deep):\", model.prob((\"love\", \"deep\"), \"learning\"))\n","print(\"P(to | i, love):\", model.prob((\"i\", \"love\"), \"coding\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1rnkZ04ly8h-","executionInfo":{"status":"ok","timestamp":1743585730293,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"a6e9bf79-5b20-419b-de63-6610f7c2aac2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["P(learning | love, deep): 0.5\n","P(to | i, love): 0.08333333333333333\n"]}]},{"cell_type":"code","source":["model.prob((\"to\", \"learn\"), \"machine\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gSKLrhNIzDXq","executionInfo":{"status":"ok","timestamp":1743585760303,"user_tz":-60,"elapsed":32,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"db49f32c-c7ab-40eb-d85b-adeba703ed49"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"],"metadata":{"id":"RFmtE-iOz1JZ"},"execution_count":null,"outputs":[]}]}