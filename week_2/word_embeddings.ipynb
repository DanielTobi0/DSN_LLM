{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2bNduWnhdmniQ4VGmbnB9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Word Embeddings in Pytorch"],"metadata":{"id":"iWnnnBjZ6BYZ"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"wnMouHPQ6AU4","executionInfo":{"status":"ok","timestamp":1744448586739,"user_tz":-60,"elapsed":7316,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","source":["word_to_ix = {'hello':0, 'world':1}\n","embeds = nn.Embedding(2,5)\n","lookup_tensor = torch.tensor([word_to_ix['world']], dtype=torch.long)\n","hello_embed = embeds(lookup_tensor)"],"metadata":{"id":"8UmNYUje6LFC","executionInfo":{"status":"ok","timestamp":1744448586864,"user_tz":-60,"elapsed":117,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["hello_embed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tcJtBhY7PH-r","executionInfo":{"status":"ok","timestamp":1744448586974,"user_tz":-60,"elapsed":107,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"b0af7922-9050-4ed0-9b24-fb7c9375789b"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-0.6288, -2.7615, -0.1449, -2.0777, -1.0309]],\n","       grad_fn=<EmbeddingBackward0>)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["hello_embed.shape, embeds.weight.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OIWhnqws6Syq","executionInfo":{"status":"ok","timestamp":1744448587040,"user_tz":-60,"elapsed":46,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"7130d86c-bbed-40b7-e98a-be4c81ae0ad0"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 5]), torch.Size([2, 5]))"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["lookup_tensor"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMyPy_956UN7","executionInfo":{"status":"ok","timestamp":1744448587053,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"347be9e4-042b-43cd-9839-105559a0c5d4"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## Word2Vec Embeddings\n","\n","This transforming words into dense vector representations, capturing semantic relationships.\n","\n","---\n","\n","### How Are Vectors Learned?\n","\n","The idea is simple but brilliant:\n","\n","- Initialize word vectors randomly.  \n","- Use a shallow neural network (just 1 hidden layer).  \n","- Train it using **CBOW** or **Skip-Gram** on a large corpus.  \n","- The model learns to predict well by adjusting the word vectors.  \n","\n","Once training is done, the weights of the hidden layer are the **word embeddings**.\n","\n","> The model doesn't care about the actual predictions; it just wants embeddings that are useful for making them.\n","\n","---\n","\n","### How Does It Work?\n","\n","There are two main architectures for Word2Vec:\n","\n","---\n","\n","#### 1. CBOW (Continuous Bag of Words)\n","\n","Predicts the **target word** from **surrounding context words**.\n","\n","It's like:\n","\n","> Given: `\"The ___ barks at night\"` → Predict: `\"dog\"`\n","\n","CBOW uses **context to predict the word**.\n","\n","---\n","\n","#### 2. Skip-Gram\n","\n","Does the reverse: uses the **target word** to predict **context words**.\n","\n","It's like:\n","\n","> Given: `\"dog\"` → Predict: `\"The\", \"barks\", \"at\", \"night\"`\n","\n","Skip-Gram is **more powerful** for capturing **rare words** or **richer semantic structure**."],"metadata":{"id":"SD6GC7vZ7Gf5"}},{"cell_type":"markdown","source":["## sample corpus"],"metadata":{"id":"R5TXOzbB_9Sr"}},{"cell_type":"code","source":["corpus = [\n","    \"the quick brown fox jumped over the lazy dog\",\n","    \"i love natural language processing\",\n","    \"word2vec is a great tool for embeddings\"\n","]"],"metadata":{"id":"Mi4YeT0m6t4t","executionInfo":{"status":"ok","timestamp":1744448639131,"user_tz":-60,"elapsed":26,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Preprocessing and Vocab Building"],"metadata":{"id":"Ys7sQx9iABpt"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from collections import Counter\n","import random\n","import numpy as np\n","\n","\n","tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n","all_words = [word for sentence in tokenized_corpus for word in sentence]\n","\n","vocab = list(set(all_words))\n","word2idx = {w: i for i, w in enumerate(vocab)}\n","idx2word = {i: w for i, w in word2idx.items()}\n","vocab_size = len(vocab)\n","\n","print(f'vocab size: {vocab_size}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4dUsasKL__gq","executionInfo":{"status":"ok","timestamp":1744448673317,"user_tz":-60,"elapsed":28,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"1d457fc8-7efa-4768-c770-12e5b2cf08d7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["vocab size: 20\n"]}]},{"cell_type":"markdown","source":["## Generate skip-gram pairs"],"metadata":{"id":"JjfdfWLGA7nQ"}},{"cell_type":"code","source":["def generate_skip_gram_pairs(tokenized_corpus, window_size=2):\n","    pairs = []\n","    for sentence in tokenized_corpus:\n","        for i, target_word in enumerate(sentence):\n","\n","            left_sentence = sentence[max(i - window_size, 0): i]\n","            right_sentence = sentence[i+1: i + window_size + 1]\n","            context_window = left_sentence + right_sentence\n","\n","            for context_word in context_window:\n","                pairs.append((target_word, context_word))\n","    return pairs\n","\n","pairs = generate_skip_gram_pairs(tokenized_corpus)\n","print(\"Sample pairs:\", pairs[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TDkfOWnASa9","executionInfo":{"status":"ok","timestamp":1744448675464,"user_tz":-60,"elapsed":34,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"78c5ac9f-7dbd-4129-912e-31770ed9c692"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Sample pairs: [('the', 'quick'), ('the', 'brown'), ('quick', 'the'), ('quick', 'brown'), ('quick', 'fox')]\n"]}]},{"cell_type":"code","source":["class Word2Vec(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim):\n","        super(Word2Vec, self).__init__()\n","        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim) # target word\n","        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim) # neighbour/context word\n","\n","\n","    def forward(self, target, context, negative_samples):\n","        target_embed = self.target_embeddings(target)           # (batch_size, embed_dim)\n","        context_embed = self.context_embeddings(context)        # (batch_size, embed_dim)\n","        negative_embed = self.context_embeddings(negative_samples)  # (batch_size, K, embed_dim)\n","\n","        # positive score\n","        pos_score = torch.mul(target_embed, context_embed).sum(dim=1)\n","        pos_loss = torch.log(torch.sigmoid(pos_score))\n","\n","        # negative score\n","        neg_score = torch.bmm(negative_embed, target_embed.unsqueeze(2)).squeeze()\n","        neg_loss = torch.log(torch.sigmoid(-neg_score)).sum(dim=1)\n","\n","        return - (pos_loss + neg_loss).mean()"],"metadata":{"id":"CwHKQNj0Bzpm","executionInfo":{"status":"ok","timestamp":1744448691457,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def get_negative_samples(batch_size, K):\n","    return torch.randint(0, vocab_size, (batch_size, K))\n","\n","get_negative_samples(5, 3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1i6uGf-B6t-c","executionInfo":{"status":"ok","timestamp":1744448710414,"user_tz":-60,"elapsed":28,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"b4e64480-38b9-4ebd-f8f2-6906c6331dfd"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[18, 15, 13],\n","        [ 9, 11, 17],\n","        [ 1,  7,  6],\n","        [ 3,  7,  7],\n","        [ 8,  2,  2]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["embedding_dim = 50\n","model = Word2Vec(vocab_size, embedding_dim)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","# helper func to get negative samples\n","def get_negative_samples(batch_size, K):\n","    return torch.randint(0, vocab_size, (batch_size, K))\n","\n","indexed_pairs = [(word2idx[t], word2idx[c]) for t, c in pairs]\n","\n","epochs = 100\n","batch_size = 16\n","K = 5  # number of negative samples\n","\n","for epoch in range(epochs):\n","    total_loss = 0\n","    random.shuffle(indexed_pairs)\n","    for i in range(0, len(indexed_pairs), batch_size):\n","        batch = indexed_pairs[i:i+batch_size]\n","        if len(batch) == 0:\n","            continue\n","        target_batch = torch.tensor([t for t, _ in batch])\n","        context_batch = torch.tensor([c for _, c in batch])\n","        negative_batch = get_negative_samples(len(batch), K)\n","\n","        optimizer.zero_grad()\n","        loss = model(target_batch, context_batch, negative_batch)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    if epoch % 10 == 0:\n","        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ksZIUtGbBmq6","executionInfo":{"status":"ok","timestamp":1744448769026,"user_tz":-60,"elapsed":7927,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"65e6844c-f45f-4e6f-a0d2-3036ca8599eb"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, Loss: 106.0953\n","Epoch 10, Loss: 41.7375\n","Epoch 20, Loss: 24.6760\n","Epoch 30, Loss: 12.0751\n","Epoch 40, Loss: 7.8676\n","Epoch 50, Loss: 8.8930\n","Epoch 60, Loss: 7.4441\n","Epoch 70, Loss: 7.4629\n","Epoch 80, Loss: 7.1671\n","Epoch 90, Loss: 7.1041\n"]}]},{"cell_type":"code","source":["def get_embedding(word):\n","    idx = word2idx[word]\n","    return model.target_embeddings.weight[idx].detach().numpy()\n","\n","print(\"Embedding for 'language':\", get_embedding('language')[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uFVyk25-BnVq","executionInfo":{"status":"ok","timestamp":1744448769038,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"56a4ff9a-582b-4aa9-e93e-2fcbcd98ec57"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Embedding for 'language': [-0.11118357 -0.41420472  0.06129085 -1.1068505   1.3873903 ]\n"]}]},{"cell_type":"code","source":["# check the similarity between the 2 words\n","wordvec = get_embedding('word2vec')\n","quick = get_embedding('quick')\n","\n","np.dot(wordvec, quick) / (np.linalg.norm(wordvec) * np.linalg.norm(quick))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xI-PpFU8CX_4","executionInfo":{"status":"ok","timestamp":1744448769048,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"3659b302-f86a-4ef8-fe61-2fe13856288c"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.float32(0.20346515)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":[],"metadata":{"id":"IKa8ENpyCcUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cbwwh67QE3ap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KqxLFR-7E3XU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install numpy==1.23.5 -qq\n","!pip install gensim -qq"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BI_fUuaGE3RK","executionInfo":{"status":"ok","timestamp":1744448857246,"user_tz":-60,"elapsed":17496,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"c362b5ab-e720-420d-ce39-69744ac254de"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n","pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n","jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n","imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n","albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["from gensim.models import Word2Vec\n","import numpy as np\n","\n","corpus = [\n","    \"the quick brown fox jumped over the lazy dog\",\n","    \"i love natural language processing\",\n","    \"word2vec is a great tool for embeddings\"\n","]\n","\n","tokenized = [sentence.lower().split() for sentence in corpus]\n","\n","model = Word2Vec(\n","    sentences=tokenized,\n","    vector_size=50,\n","    window=2,\n","    min_count=1,\n","    sg=1, # 1 = Skip-Gram; 0 = CBOW\n","    negative=5,\n","    epochs=16\n",")\n","\n","word_vectors = model.wv"],"metadata":{"id":"guihwm_qE3Op","executionInfo":{"status":"ok","timestamp":1744448897018,"user_tz":-60,"elapsed":1820,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["word_vectors.similarity('word2vec', 'quick')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1e5PIN95E7OX","executionInfo":{"status":"ok","timestamp":1744448926683,"user_tz":-60,"elapsed":17,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"a599deb3-752c-43e0-9db4-8685fa94645c"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.03305311"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["word_vectors.most_similar(positive=['natural', 'processing'], negative=['language'], topn=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AIjqA3G5E9xZ","executionInfo":{"status":"ok","timestamp":1744448979526,"user_tz":-60,"elapsed":41,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"b306264c-e86c-43ff-d19e-4b550c0c08ff"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('jumped', 0.22528912127017975)]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["print(\"Top 5 words similar to 'language':\")\n","for word, score in word_vectors.most_similar('language', topn=5):\n","    print(f\"{word} ({score:.2f})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBcCiE3yE_BO","executionInfo":{"status":"ok","timestamp":1744448990723,"user_tz":-60,"elapsed":49,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}},"outputId":"e7d6bff2-70d4-4408-e11a-61da32d128ef"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 5 words similar to 'language':\n","brown (0.19)\n","word2vec (0.19)\n","natural (0.17)\n","lazy (0.12)\n","jumped (0.10)\n"]}]},{"cell_type":"code","source":["# Note: this was trained on an extremly small dataset, but the same principle applies to a larger dataset"],"metadata":{"id":"7gVPbgEpFxYo","executionInfo":{"status":"ok","timestamp":1744449045351,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniel Tobi","userId":"10779748150936298898"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nhtItcwDRAGR"},"execution_count":null,"outputs":[]}]}